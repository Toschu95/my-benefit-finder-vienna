{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e6a3656-58e6-40c3-9084-fe3437c5a582",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scrape_page' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 100\u001b[0m\n\u001b[1;32m     98\u001b[0m sitemap_url \u001b[38;5;241m=\u001b[39m get_sitemap(domain)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sitemap_url:\n\u001b[0;32m--> 100\u001b[0m     scrape_page(sitemap_url)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m⚠️ Keine Sitemap für \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdomain\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, starte Crawling...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scrape_page' is not defined"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import gzip\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from PyPDF2 import PdfReader\n",
    "import hashlib\n",
    "\n",
    "# Konfigurationswerte laden\n",
    "with open(\"config.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "KEYWORDS = config[\"keywords\"]\n",
    "MAIN_DOMAINS = config[\"domains\"]\n",
    "MAX_PDF_PAGES = config[\"max_pdf_pages\"]\n",
    "MAX_IMAGE_SIZE = tuple(config[\"max_image_size\"])\n",
    "STORAGE_PATHS = config[\"storage_paths\"]\n",
    "\n",
    "# Sicherstellen, dass Verzeichnisse existieren\n",
    "for path in STORAGE_PATHS.values():\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# Lade URLs aus der vorherigen Scraping-Stufe\n",
    "if os.path.exists(\"extrahierte_urls.json\"):\n",
    "    with open(\"extrahierte_urls.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        extracted_urls = json.load(f)\n",
    "else:\n",
    "    extracted_urls = {domain: [f\"https://{domain}\"] for domain in MAIN_DOMAINS}\n",
    "\n",
    "def get_hash(content):\n",
    "    \"\"\"Berechnet den Hash-Wert einer Datei oder eines Textes, um Duplikate zu vermeiden.\"\"\"\n",
    "    return hashlib.md5(content.encode()).hexdigest() if isinstance(content, str) else hashlib.md5(content).hexdigest()\n",
    "\n",
    "def extract_main_text(html):\n",
    "    \"\"\"Extrahiert nur den Hauptinhalt einer Webseite.\"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    for tag in [\"script\", \"style\", \"header\", \"footer\", \"nav\", \"aside\"]:\n",
    "        for element in soup.find_all(tag):\n",
    "            element.extract()\n",
    "    return soup.get_text().strip()\n",
    "\n",
    "def save_text(url, text):\n",
    "    \"\"\"Speichert den extrahierten Text komprimiert als GZIP.\"\"\"\n",
    "    filename = f\"{STORAGE_PATHS['text']}{urlparse(url).netloc}.txt.gz\"\n",
    "    with gzip.open(filename, \"wt\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "    print(f\"Gespeicherter Text: {filename}\")\n",
    "\n",
    "def download_pdf_if_relevant(url):\n",
    "    \"\"\"Lädt PDFs herunter, wenn sie relevante Keywords enthalten.\"\"\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    pdf_content = response.content\n",
    "    pdf_reader = PdfReader(BytesIO(pdf_content))\n",
    "\n",
    "    for page in pdf_reader.pages[:MAX_PDF_PAGES]:  # Bis zu 5 Seiten prüfen\n",
    "        text = page.extract_text()\n",
    "        if text and any(keyword.lower() in text.lower() for keyword in KEYWORDS):\n",
    "            filename = f\"{STORAGE_PATHS['pdfs']}{url.split('/')[-1]}\"\n",
    "            with open(filename, \"wb\") as f:\n",
    "                f.write(pdf_content)\n",
    "            print(f\"PDF gespeichert: {filename}\")\n",
    "            return True\n",
    "    print(f\"PDF verworfen: {url}\")\n",
    "    return False\n",
    "\n",
    "def download_and_compress_image(url):\n",
    "    \"\"\"Lädt Bilder herunter, konvertiert sie zu WebP & speichert sie komprimiert.\"\"\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    image = Image.open(BytesIO(response.content))\n",
    "    image.thumbnail(MAX_IMAGE_SIZE)\n",
    "\n",
    "    filename = f\"{STORAGE_PATHS['images']}{url.split('/')[-1].split('.')[0]}.webp\"\n",
    "    image.save(filename, \"WEBP\", quality=80)\n",
    "    print(f\"Bild gespeichert: {filename}\")\n",
    "\n",
    "def scrape_page(url):\n",
    "    \"\"\"Scrapt eine Webseite & speichert relevante Inhalte (Text, PDFs, Bilder).\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # 1️⃣ Haupttext extrahieren & speichern\n",
    "        main_text = extract_main_text(response.text)\n",
    "        if any(keyword.lower() in main_text.lower() for keyword in KEYWORDS):\n",
    "            save_text(url, main_text)\n",
    "\n",
    "        # 2️⃣ PDFs extrahieren & speichern\n",
    "        for link in soup.find_all(\"a\", href=True):\n",
    "            file_url = urljoin(url, link[\"href\"])\n",
    "            if file_url.endswith(\".pdf\"):\n",
    "                download_pdf_if_relevant(file_url)\n",
    "\n",
    "        # 3️⃣ Bilder extrahieren & speichern\n",
    "        for img in soup.find_all(\"img\", src=True):\n",
    "            img_url = urljoin(url, img[\"src\"])\n",
    "            if any(ext in img_url.lower() for ext in [\".jpg\", \".jpeg\", \".png\"]):\n",
    "                download_and_compress_image(img_url)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Scraping von {url}: {e}\")\n",
    "\n",
    "# Scraping für jede gefundene URL ausführen\n",
    "for domain, urls in extracted_urls.items():\n",
    "    for url in urls:\n",
    "        scrape_page(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab72005-bbef-4fb5-a7e5-3d14865c650d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16f45d2-2fe1-4cab-a153-37c14674a8da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
